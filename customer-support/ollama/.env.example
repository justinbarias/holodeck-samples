# Ollama Configuration
# No API key required - Ollama runs locally

# Ensure Ollama is running with required models:
# ollama pull llama3.1:8b
# ollama pull nomic-embed-text:latest

# Default Ollama endpoint (change if using remote Ollama)
OLLAMA_HOST=http://localhost:11434
